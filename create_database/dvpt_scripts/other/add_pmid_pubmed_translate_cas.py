from datasets import load_dataset, Dataset
from tqdm import tqdm

# Ã‰tape 1 : Chargement des datasets
print("ðŸ”„ Chargement des jeux de donnÃ©es...")
pubmed_big_all = load_dataset("rntc/pubmed-comm", split="train", streaming=True)
pubmed_big = [row for row in pubmed_big_all if row["document_type"] == "Clinical Case"]
ds = Dataset.from_list(pubmed_big)
ds.push_to_hub("clairedhx/pubmed-comm-clinical-cases")

pubmed_small = load_dataset("rntc/clinical-pubmed-translate", split="train")

# Ã‰tape 2 : CrÃ©ation de lâ€™index {text -> id}
print("ðŸ” Indexation du petit dataset...")
target_texts = set(row["text"] for row in pubmed_small)
text_to_id = {}

for row in tqdm(ds, desc="Indexation filtrÃ©e"):
    if row["text"] in target_texts:
        text_to_id[row["text"]] = row["id"]
        if len(text_to_id) >= len(target_texts):  # Early stop
            break

# Ã‰tape 3 : Ajout des IDs correspondants au petit dataset
print("ðŸ“Œ Ajout des IDs correspondants...")
enriched_rows = []
for row in tqdm(pubmed_small, desc="Enrichissement"):
    match_id = text_to_id.get(row["text"])
    new_row = dict(row)
    new_row["pubmed_comm_id"] = match_id  # peut Ãªtre None si non trouvÃ©
    enriched_rows.append(new_row)

# Ã‰tape 4 : CrÃ©ation dâ€™un nouveau dataset
enriched_dataset = Dataset.from_list(enriched_rows)

# Ã‰tape 5 : Push vers Hugging Face
print("ðŸš€ Push vers le hub HF...")
enriched_dataset.push_to_hub("clairedhx/clinical-pubmed-translate-with-id")

print("âœ… Fini ! Nouveau dataset en ligne.")
